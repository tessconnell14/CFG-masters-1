# -*- coding: utf-8 -*-
"""Copy of developer_survey_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FXPnx7EuNvy9P9699vea-SO9ltaajreE

## Task 3: EDA
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
from sklearn.linear_model import LinearRegression

df = pd.read_csv('/content/drive/My Drive/CFG_masters/survey_results_public.csv')

pd.set_option('display.max_columns', None)
df.head()

df.tail()

df.sample()

df.shape

"""**Observations**
*   There are a number of missing values in the text
*   There are a number of columns which are irrelevent to our analysis so can be removed


"""

df.describe()

"""**Observations**
- 'CompTotal' column is in different currencies so varies widely and is difficult to compare
- 'ConvertedCompYearly' is compensation converted into USD so will be used for comparison
- The range in converted salaries is vast (from `$1` to `$74,351,430`)
- However, 75% of respondents get paid `$121,641` or less so `$74,351,430` is an outlier
- The mean compensation is `$103,110` so `$1` is also an outlier
- The mean (`$103,110) and median (`$74,963`) average pay vary significantly, meaning that the mean is being influenced by very high outliers and the median may be a better measure of average pay in this case
- The work experience shows a good range from 0 to 50 years, meaning there will be a good representation of experience
- However, 75% of respondants actually have fewer than 16 years' experience so 50 years is an outlier (the mean being 11.4 years and median being 9 years).

"""

df.columns

df.dtypes

pd.set_option('display.max_rows', None)
missing = df.isna().sum()
missing = missing[missing > 0]
missing = missing.sort_values(ascending = False)
missing

"""**Observations**
- Most columns have missing values
- In some columns, almost all values are missing so they are not useable, but many also do not relate to our question
- Almost half of the survey respondents didn't share their salary. Some of these may not be working developers. For the others we will need to consider how we fill these values. The same goes for work experience.

## Initial Visualisations
"""

# Histogram for the ConvertedCompYearly column (non-zero values)
plt.figure(figsize=(10, 6))
df['ConvertedCompYearly'].hist(bins=30)
plt.xlabel('ConvertedCompYearly')
plt.ylabel('Frequency')
plt.title('Histogram of ConvertedCompYearly')
plt.show()

df_filtered = df[df['ConvertedCompYearly'] < 1000000]
plt.figure(figsize=(10, 6))
df_filtered['ConvertedCompYearly'].hist(bins=50)
plt.xlabel('Converted Compensation Yearly $')
plt.ylabel('Frequency')
plt.title('Histogram of Yearly Compensation Distribution')
plt.gca().xaxis.set_major_locator(MultipleLocator(100000))
plt.savefig('compensation_under_1m.png')
plt.show()

"""**Notes:**
- Very high outliers make the first histogram unreadable so we excluded anything over `Â£1m` to get a better overview of the distribution.
- The second histogram shows that the salary data is right skewed, with most people earning between `$0` and `$100,000`
- Although there are salaries over `$500,000`, these are outliers
- On closer inspection (below) only 303 people (around 0.7% of respondents who shared their salaries) earn over `$500,000`
"""

high_earners = df[df['ConvertedCompYearly']>=500000]
high_earners.shape

"""##Task 4: Data Cleaning

Steps:
- remove unwanted columns
- handle missing values
- handle outliers
- handle duplicates
"""

df = df[['ResponseId','MainBranch', 'Age', 'Employment', 'RemoteWork',
       'CodingActivities', 'EdLevel', 'LearnCode', 'LearnCodeOnline',
       'LearnCodeCoursesCert', 'YearsCode', 'YearsCodePro', 'DevType',
       'OrgSize', 'PurchaseInfluence', 'TechList', 'Country', 'LanguageHaveWorkedWith',
       'DatabaseHaveWorkedWith','PlatformHaveWorkedWith',
       'WebframeHaveWorkedWith','MiscTechHaveWorkedWith',
       'ToolsTechHaveWorkedWith','NEWCollabToolsHaveWorkedWith',
       'AISearchHaveWorkedWith', 'AIDevHaveWorkedWith',
       'NEWSOSites', 'SOVisitFreq', 'SOPartFreq',
       'SOAI', 'AISelect', 'AISent', 'AIAcc', 'AIBen','AIToolCurrently Using',
       'ICorPM','WorkExp','Industry','ConvertedCompYearly']]

df.head()

missing = df.isna().sum()
missing

df['Employment'].values

df = df.dropna(subset=['Employment'])
df['Employment'].value_counts()

# Create dataframe containing only the employed respondents
employed = df[df['Employment'].str.contains('Employed, full-time') | df['Employment'].str.contains('Employed, part-time') | df['Employment'].str.contains('Independent contractor')  ]
employed.shape

# Select only people employed as developers
employed = employed[employed['MainBranch']=="I am a developer by profession"]
employed.shape

# Check missing values in employed df
employed.isna().sum()

# Remove any columns where more than half of the data is missing and which are not essential for our enquiry
threshold = 0.5
missing_proportion = employed.isna().mean()
employed = employed.loc[:, missing_proportion <= threshold]
employed.isna().sum()

employed[['Age','EdLevel','YearsCode','YearsCodePro','WorkExp','ConvertedCompYearly']].head(100)

employed[['Age','EdLevel','YearsCode','YearsCodePro','WorkExp','ConvertedCompYearly']].dtypes

def convert_to_numeric(years):
    if years == 'Less than 1 year':
        return 0.5
    elif years == 'More than 50 years':
        return 50
    else:
        return float(years)

employed['YearsCode'] = employed['YearsCode'].apply(convert_to_numeric)
employed['YearsCodePro'] = employed['YearsCodePro'].apply(convert_to_numeric)

# Calculate the mean difference between YearsCode and YearsCodePro to be used to fill missing values
difference_years = employed['YearsCode'] - employed['YearsCodePro']
av_difference_years = difference_years.mean()
print(av_difference_years)

"""**Handling missing data:**
- There is still a vast amount of data missing in the employed dataframe
- Missing data in each column will need to be handled differently as they are different datatypes/have different purposes

**Most relevant columns to our study:**
- YearsCode: Will be filled with the median                         
- YearsCodePro: Will be filled with `YearsCode - av_difference_years` (see above)
- WorkExp: Missing data will be filled with the 'YearsCodePro' data as, on examination, the answers for these questions are predominantly similar (with the exception of career changers)
- ConvertedCompYearly: Missing data in these columns will be filled with the median, but machine learning will be used to get better estimates later

**Other columns:**
<br>
Other columns where data is categorical (e.g. organisation size, industry etc), missing vals will be filled with the mode
"""

# Fill YearsCode NaN values
employed['YearsCode']=employed['YearsCode'].fillna(employed['YearsCode'].median())

# Fill YearsCodePro NaN values
employed['YearsCodePro']=employed['YearsCodePro'].fillna(employed['YearsCode']-av_difference_years)

# Fill WorkExp NaN values
employed['WorkExp'] = employed['WorkExp'].fillna(employed['YearsCodePro'])

# Fill ConvertedCompYearly NaN values

employed['ConvertedCompYearly'] = employed['ConvertedCompYearly'].fillna(employed['ConvertedCompYearly'].median())

employed.isna().sum().sort_values()

employed.columns

# Fill NaN values in categorical data columns with mode
def fill_missing_vals(df, column):
  mode_val = df[column].mode()[0]
  df[column].fillna(mode_val, inplace=True)


categorical_columns = ['LearnCode', 'LearnCodeOnline','DevType','OrgSize','RemoteWork',
                       'PurchaseInfluence','TechList','LanguageHaveWorkedWith',
                       'DatabaseHaveWorkedWith','PlatformHaveWorkedWith',
                       'WebframeHaveWorkedWith', 'MiscTechHaveWorkedWith',
                       'ToolsTechHaveWorkedWith', 'NEWCollabToolsHaveWorkedWith',
                       'AISearchHaveWorkedWith', 'NEWSOSites', 'SOVisitFreq', 'SOPartFreq',
                       'SOAI', 'AISent', 'AIBen', 'ICorPM','Industry', 'CodingActivities'
                       ]

for column in categorical_columns:
  fill_missing_vals(employed, column)

employed.isna().sum()

# Check if there are any duplicates in the data
employed.duplicated().sum()

path = 'survey_data_clean.csv'
employed.to_csv(path, index=False)